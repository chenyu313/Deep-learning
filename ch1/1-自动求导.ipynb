{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导\n",
    "### 什么是自动求导\n",
    "自动求导(AUTOMATIC DIFFERENTIATION 简称 AD) 或许是目前你从未听过且最有用的计算技术之一。如果你的工作涉及到实数计算，那么理解 AD 或许将会对你的工作有所帮助。  \n",
    "自动求导，也被称为算法求导(algorithmic differentiation)或计算求导(computational differentiation)，是一组使用数值方法对某个函数，通过编程的方式进行求导的技术。可以求任意阶的导数，求导过程是自动的，并且能够保证足够的精度，以及较小的时间复杂度。\n",
    "\n",
    "torch.autograd是 PyTorch 的自动差分引擎，可为神经网络训练提供支持。 在本节中，您将获得有关 Autograd 如何帮助神经网络训练的概念性理解\n",
    "\n",
    "### 背景\n",
    "神经网络（NN）是在某些输入数据上执行的嵌套函数的集合。 这些函数由参数（由权重和偏差组成）定义，这些参数在 PyTorch 中存储在张量中。\n",
    "\n",
    "训练 NN 分为两个步骤：\n",
    "* 正向传播：在正向传播中，NN 对正确的输出进行最佳猜测。 它通过其每个函数运行输入数据以进行猜测。\n",
    "* 反向传播：在反向传播中，NN 根据其猜测中的误差调整其参数。 它通过从输出向后遍历，收集有关函数参数（梯度）的误差导数并使用梯度下降来优化参数来实现。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们来看一个训练步骤。 对于此示例，我们从torchvision加载了经过预训练的 resnet18 模型。 我们创建一个随机数据张量来表示具有 3 个通道的单个图像，高度&宽度为 64，其对应的label初始化为一些随机值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们通过模型的每一层运行输入数据以进行预测。 这是正向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # 正向传播\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用模型的预测和相应的标签来计算误差（loss）。 下一步是通过网络反向传播此误差。 当我们在误差张量上调用.backward()时，开始反向传播。 然后，Autograd 会为每个模型参数计算梯度并将其存储在参数的.grad属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # 反向传播"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们加载一个优化器，在本例中为 SGD（随机梯度下降），学习率为 0.01，动量为 0.9。 我们在优化器中注册模型的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们调用.step()启动梯度下降。 优化器通过.grad中存储的梯度来调整每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #梯度下降\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd 的微分\n",
    "让我们来看看autograd如何收集梯度。 我们用requires_grad=True创建两个张量a和b。 这向autograd发出信号，应跟踪对它们的所有操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAAAmCAYAAAAfrNPMAAAEOElEQVR4nO2a7XGrOBiFz93ZAmSXIDoAl4BTAVACdIApAVIBdglABUglEFcAKkGmA+2PjDUhcbCDjcnd0fOLD411PBxeHUn8UUopGAzf8M/SAgy/G2MQwyjGIIZRjEEMoxiDGEYxBjGMYgxiGMUYxDCKMYhhlH+XFvBbKcsShBD0fQ/OOdI0xWq1WloWyrKElBJCCPR9j/1+P2+HynARQog+9jxPpWm6oJp32rYd6PA8T8VxPGufZoj5hqZp9LEQAq7rLqjmnb7vURSFPnddF8fjcdY+HzbEnE4nHA4HANClWQjxa0rzT7EsC13XoaoqBEEAx3Ge0m/XdUiSBFVVoW1bWJal7zmOg7e3N30uhIBt2/MKekQZyvNcua6rpJSD623bKkLIl+tzk+e5yvNcpWl61/AgpVR1XaswDJ/6H5qmUdcejZRSUUpn13W3QeI4Vq7rTr7/aOI4VnmeD64BuCtDhGH41P9wfuHG8DxPNU0zu5a7MghjDIfDYTAufmaz2YBzfk83P6aqqsE5IWSQKa5RliW2260+t20bQoiH6bsG53w082RZhiRJ4DgOuq6bVctdBgmC4GrGoJQCwGDsnJMsy8AY0+en0wl93/8oZBJCBu2PxyM8z3uozjE4599mC8YYbNvWmejzy/BoJofU3W6Hvu8RRdFou/ObdzbKszkcDrBt+6LOLMsAvGujlIIQAsuysN1uIYTQawyEEN12brquQ9/36PseZVlCCAFKKXzfR9d1eHl5GbSP43hWPZMNUlXVTW/leXgZqzJRFEFKeVO/lNKbHtZ+v9dTwEvVa7vdwvM8bRzf9yGE0G2vGX8uOOcghMC2bT2DWa1WcF0XlmVBPfsL0anhBTcGP0qp8jxvajd3c2mGdSkEPmPR6RYu6SCEqLquF9EzqYKcTicA14cNxhiEEKjreko3DyGKIl3tztUhSRKkaTpoxzkfDdvXYIzh9fX1prYfK9dnLuno+x7r9XqytruY6ixCyJfp5GcopSoMw6ldTOLS9C8MQwVASSlV27YKgGrbVt+/Zd3hGUgpv+goimJRbZMzSBAE4JwjiqLBKqrrunAcB1EUwbbtmzaTHpVBvlv5PP/2arXSofnjCuXHaSVjbDDFfSZSyi9VuSiKL9XuqUx11nklr2maQSXJ83zR8TyO40HekFIqQsggL33ciJNSKtu2td5rVXFuKKX6uK5rZdv2gmqU+qPUfbF4t9vpzazzZlJRFPoN7bpu8LY+gyzLIKXEer1G0zQIggC+7+v7jLHBLIxSqqfDQRAsunfEGAPnHJvNBkII7Ha7xbQAwN0G+Yzv+yjLEsB7mOWcDx6O4e/i4dv9x+NRz3KSJDHm+MuZ5XuQzWYDx3EWWz01PI6HGyQMQ53Glx4/Dffz8Axi+H9hPjk0jGIMYhjFGMQwijGIYRRjEMMoxiCGUYxBDKP8B1Kcj3+OJg9oAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们从a和b创建另一个张量Q。  \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGcAAABECAYAAACceSthAAADT0lEQVR4nO3c73GiQBgG8IebK2CTEpYOkBJIB0gJ2EGwBOmAWAJjBUAJSAWEEhY62PvgScLgJcqt+Gre3yclDtnhcf+wvKOltdZgJP26dQPYv3E4hHE4hHE4hHE4hHE4hHE4hHE4hP02daL9fo/1eo2iKFDXNWzbHvy9bVtst1sAgBACXdehaRpsNhs8PT2ZasZj0QZlWaaFEKPjSZJoz/O0UmpwvK5rLYQYHWcHRoe1qqrged7gWBRF2O12yPN81ENs20YYhgiCwGQzHofJpD3P00mS9O+PPemrnpGmqTbcjIdh9KoA0HVd9++FEIOwTinLUgPQZVmabMpD+K8FQRzHAAApJYDDRH9cCERRhK7rsFqtvjxH0zSDc7APk8N5eXmB7/v9xV8ul4P5ZrfbjeafU4qiAABesZ0waUHw9vYGAKNe4bpu/7ppmrPD8X1/SjMe35Sx8NRcIoTo5w2llAag0zT98jxZlo3mKfbh4nDquh5d0OOk/tk5iwEppQ7D8NIm/BgXD2td1wHAYAegKIp+CMvzHAAQBEE/n7RtiziOEccx9vs9gMOQ6DhOP0SyE6Yk+nkXQCmlHcfRr6+vWmvd9xallJZS6rIsBz0oSRLt+37/efZvltaXF3jkeT5YjUkpsd1u4TgOgiAYrLyiKOoXB13XIU1TpGna97z39/fRPhz7a+5vg+/7/Wul1LeLhmvLskw7jkPyJnj2RwZVVaFtWwDAer3GcrmcuwkADj02jmM8Pz+jqqqbtOE7xh4ZXMJ1XQghbrrhads2oii62f8/x+w9JwxDKKUgpSR/cW5t0oLg0ViWhbIssVgsbt2UgZsMa1OtVisopc76rJSy35i9V3cVzk+7Yb04HMuyrtGOHo+yHy4Ohy/efO5qWOM5h7Brzjnnhj6nuwrHpLZtsdls+sfkx5q7IAjILKmN3edwUeEVmNyo46JCs7iokDKTSXNRoVlcVEgYFxUSxkWFhHFRIWVTxkIuKpwHFxUSxkWFlE1JlIsK58FFhZTN/W2gVlRI2Y8tKrwHN/mRCNd1sVgseFfgG1xUSBgXFRLGv31DGIdDGIdDGIdDGIdDGIdDGIdDGIdDGIdDGIdDGIdDGIdDGIdDGIdDGIdDGIdD2B/O0gklM1OsEwAAAABJRU5ErkJggg=="
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设a和b是神经网络的参数，Q是误差。 在 NN 训练中，我们想要相对于参数的误差，即  \n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "当我们在Q上调用.backward()时，Autograd 将计算这些梯度并将其存储在各个张量的.grad属性中。\n",
    "\n",
    "我们需要在Q.backward()中显式传递gradient参数，因为它是向量。 gradient是与Q形状相同的张量，它表示Q相对于本身的梯度，即\n",
    "\n",
    "![image.png](attachment:image-2.png)\n",
    "\n",
    "同样，我们也可以将Q聚合为一个标量，然后隐式地向后调用，例如Q.sum().backward()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度现在沉积在a.grad和b.grad中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================================================================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们想对函数y=2x^Tx关于列向量求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在计算y关于x的梯度之前，我们需要一个地方来存储梯度。（梯度的本意是一个向量，表示某函数在该点处的方向导数沿该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "x.grad #x的梯度默认值是None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=2*torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过调用反向传播函数来自动计算y关于x每个分量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() #这里反向转播就是自动求导y=2*x*x求导后为4*x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#所以这里可以验证一下\n",
    "x.grad==4*x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算x的另一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在默认情况下，Pytorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y=x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=x*x\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将某些计算移到记录的计算图之外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=x*x\n",
    "u=y.detach() #这里相当于对y进行不对x进行求导，相当于把u变成了一个常数\n",
    "z=u*x\n",
    "z.sum().backward()\n",
    "x.grad==u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
